{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Datasets for Joins\n",
    "\n",
    "Let us analyze data sets that are going to be used for joins.\n",
    "* We will use January 2008 airlines data which have all relevant flight details.\n",
    "* Let us read and review the airlines data quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Joining Data Sets'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark. \\\n",
    "    read. \\\n",
    "    parquet(\"/public/airlines_all/airlines-part/flightmonth=200801\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will be using another data set to get details about airports. Details include information such as State, City etc for a given airport code.\n",
    "* Let us analyze the Dataset to confirm if there is header and also how the data is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    text(airportCodesPath). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Data is tab separated.\n",
    " * There is header for the data set.\n",
    " * Dataset have 4 fields - **Country, State, City, IATA**\n",
    "    \n",
    "    \n",
    "Create DataFrame airport_codes applying appropriate Schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = spark. \\\n",
    "    read. \\\n",
    "    option(\"sep\", \"\\t\"). \\\n",
    "    option(\"header\", True). \\\n",
    "    option(\"inferSchema\", True). \\\n",
    "    csv(airportCodesPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preview and Understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get schema of **airport_codes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the count of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Get the count of unique records and see if it is the same as total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    select(\"IATA\"). \\\n",
    "    distinct(). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * If they are not equal, analyze the data and identify IATA codes which are repeated more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateIATACount = airportCodes. \\\n",
    "    groupBy(\"IATA\"). \\\n",
    "    agg(count(lit(1)).alias(\"iata_count\")). \\\n",
    "    filter(\"iata_count > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|IATA|iata_count|\n",
      "+----+----------+\n",
      "| Big|         3|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicateIATACount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Filter out the duplicates using the most appropriate one and discard others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-------+----+\n",
      "|       City| State|Country|IATA|\n",
      "+-----------+------+-------+----+\n",
      "|       Hilo|    HI|    USA| Big|\n",
      "|Kailua-Kona|Hawaii|    USA| Big|\n",
      "|    Kamuela|Hawaii|    USA| Big|\n",
      "+-----------+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    filter(\"IATA = 'Big'\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "|   Alliance|   NE|    USA| AIA|\n",
      "|     Alpena|   MI|    USA| APN|\n",
      "|    Altoona|   PA|    USA| AOO|\n",
      "|   Amarillo|   TX|    USA| AMA|\n",
      "|Anahim Lake|   BC| Canada| YAA|\n",
      "|  Anchorage|   AK|    USA| ANC|\n",
      "|   Appleton|   WI|    USA| ATW|\n",
      "|     Arviat|  NWT| Canada| YEK|\n",
      "|  Asheville|   NC|    USA| AVL|\n",
      "|      Aspen|   CO|    USA| ASE|\n",
      "+-----------+-----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    filter(\"!(State = 'Hawaii' AND IATA = 'Big')\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    filter(\"!(State = 'Hawaii' AND IATA = 'Big')\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Get number of airports (IATA Codes) for each state in the US. Sort the data in descending order by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = spark. \\\n",
    "    read. \\\n",
    "    option(\"sep\", \"\\t\"). \\\n",
    "    option(\"header\", True). \\\n",
    "    option(\"inferSchema\", True). \\\n",
    "    csv(airportCodesPath). \\\n",
    "    filter(\"!(State = 'Hawaii' AND IATA = 'Big') AND Country = 'USA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCountByState = airportCodes. \\\n",
    "    groupBy(\"Country\", \"State\"). \\\n",
    "    agg(count(lit(1)).alias(\"IATACount\")). \\\n",
    "    orderBy(col(\"IATACount\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+\n",
      "|Country|State|IATACount|\n",
      "+-------+-----+---------+\n",
      "|    USA|   CA|       29|\n",
      "|    USA|   TX|       26|\n",
      "|    USA|   AK|       25|\n",
      "|    USA|   NY|       18|\n",
      "|    USA|   MI|       18|\n",
      "|    USA|   FL|       18|\n",
      "|    USA|   MT|       14|\n",
      "|    USA|   PA|       13|\n",
      "|    USA|   IL|       12|\n",
      "|    USA|   CO|       12|\n",
      "|    USA|   WY|       10|\n",
      "|    USA|   NC|       10|\n",
      "|    USA|   WI|        9|\n",
      "|    USA|   NE|        9|\n",
      "|    USA|   GA|        9|\n",
      "|    USA|   NM|        9|\n",
      "|    USA|   HI|        9|\n",
      "|    USA|   WA|        9|\n",
      "|    USA|   KS|        9|\n",
      "|    USA|   ND|        8|\n",
      "|    USA|   MO|        8|\n",
      "|    USA|   AR|        8|\n",
      "|    USA|   MA|        8|\n",
      "|    USA|   MN|        8|\n",
      "|    USA|   AZ|        8|\n",
      "|    USA|   WV|        8|\n",
      "|    USA|   IA|        8|\n",
      "|    USA|   SD|        7|\n",
      "|    USA|   ME|        7|\n",
      "|    USA|   VA|        7|\n",
      "|    USA|   LA|        7|\n",
      "|    USA|   MS|        7|\n",
      "|    USA|   OR|        7|\n",
      "|    USA|   TN|        6|\n",
      "|    USA|   AL|        6|\n",
      "|    USA|   OH|        6|\n",
      "|    USA|   IN|        6|\n",
      "|    USA|   ID|        6|\n",
      "|    USA|   SC|        6|\n",
      "|    USA|   OK|        5|\n",
      "|    USA|   KY|        4|\n",
      "|    USA|   VT|        3|\n",
      "|    USA|   NV|        3|\n",
      "|    USA|   NJ|        3|\n",
      "|    USA|   NH|        3|\n",
      "|    USA|   MD|        3|\n",
      "|    USA| null|        3|\n",
      "|    USA|   UT|        2|\n",
      "|    USA|   CT|        2|\n",
      "|    USA|   DE|        1|\n",
      "|    USA|   RI|        1|\n",
      "+-------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCountByState.show(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
