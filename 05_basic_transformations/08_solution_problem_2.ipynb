{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Solutions - Problem 2\n",
    "\n",
    "Get number of flights which are delayed in departure and number of flights delayed in arrival for each day along with number of flights departed for each day. \n",
    "\n",
    "* Output should contain 4 columns - **FlightDate**, **FlightCount**, **DepDelayedCount**, **ArrDelayedCount**\n",
    "* **FlightDate** should be of **YYYY-MM-dd** format.\n",
    "*   Data should be **sorted** in ascending order by **flightDate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Basic Transformations'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Grouping Data by Flight Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat, lpad\n",
    "airlines. \\\n",
    "  groupBy(concat(\"Year\", lit(\"-\"), \n",
    "                 lpad(\"Month\", 2, \"0\"), lit(\"-\"), \n",
    "                 lpad(\"DayOfMonth\", 2, \"0\")).\n",
    "          alias(\"FlightDate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Counts by FlightDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat, lpad, count\n",
    "\n",
    "airlines. \\\n",
    "    groupBy(concat(\"Year\", lit(\"-\"), \n",
    "                   lpad(\"Month\", 2, \"0\"), lit(\"-\"), \n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")).\n",
    "            alias(\"FlightDate\")). \\\n",
    "    agg(count(lit(1)).alias(\"FlightCount\")). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to get the count with out using agg\n",
    "# We will not be able to provide alias for aggregated fields\n",
    "from pyspark.sql.functions import lit, concat, lpad\n",
    "\n",
    "airlines. \\\n",
    "    groupBy(concat(\"Year\", lit(\"-\"), \n",
    "                   lpad(\"Month\", 2, \"0\"), lit(\"-\"), \n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")).\n",
    "            alias(\"FlightDate\")). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting total as well as delayed counts for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat, lpad, count, sum, expr\n",
    "\n",
    "airlines. \\\n",
    "    groupBy(concat(\"Year\", lit(\"-\"), \n",
    "                   lpad(\"Month\", 2, \"0\"), lit(\"-\"), \n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")).\n",
    "            alias(\"FlightDate\")). \\\n",
    "    agg(count(lit(1)).alias(\"FlightCount\"),\n",
    "        sum(expr(\"CASE WHEN IsDepDelayed = 'YES' THEN 1 ELSE 0 END\")).alias(\"DepDelayedCount\"),\n",
    "        sum(expr(\"CASE WHEN IsArrDelayed = 'YES' THEN 1 ELSE 0 END\")).alias(\"ArrDelayedCount\")\n",
    "       ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data By FlightDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(airlines.sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(airlines.orderBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat, lpad, sum, expr\n",
    "airlines. \\\n",
    "    groupBy(concat(\"Year\", lit(\"-\"), \n",
    "                   lpad(\"Month\", 2, \"0\"), lit(\"-\"), \n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")).\n",
    "            alias(\"FlightDate\")). \\\n",
    "    agg(count(lit(1)).alias(\"FlightCount\"),\n",
    "        sum(expr(\"CASE WHEN IsDepDelayed = 'YES' THEN 1 ELSE 0 END\")).alias(\"DepDelayedCount\"),\n",
    "        sum(expr(\"CASE WHEN IsArrDelayed = 'YES' THEN 1 ELSE 0 END\")).alias(\"ArrDelayedCount\")\n",
    "       ). \\\n",
    "    orderBy(\"FlightDate\"). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data in descending order by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat, lpad, sum, expr, col\n",
    "airlines. \\\n",
    "    groupBy(concat(\"Year\", lit(\"-\"), \n",
    "                   lpad(\"Month\", 2, \"0\"), lit(\"-\"), \n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")).\n",
    "            alias(\"FlightDate\")). \\\n",
    "    agg(count(lit(1)).alias(\"FlightCount\"),\n",
    "        sum(expr(\"CASE WHEN IsDepDelayed = 'YES' THEN 1 ELSE 0 END\")).alias(\"DepDelayedCount\"),\n",
    "        sum(expr(\"CASE WHEN IsArrDelayed = 'YES' THEN 1 ELSE 0 END\")).alias(\"ArrDelayedCount\")\n",
    "       ). \\\n",
    "    orderBy(col(\"FlightCount\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
