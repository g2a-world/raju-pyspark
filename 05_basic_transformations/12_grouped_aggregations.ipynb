{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Aggregations\n",
    "\n",
    "Let us go through the details related to aggregation using Spark.\n",
    "\n",
    "* We can perform total aggregations directly on Dataframe or we can perform aggregations after grouping by a key(s).\n",
    "* Here are the APIs which we typically use to group the data using a key.\n",
    "  * groupBy\n",
    "  * rollup\n",
    "  * cube\n",
    "* Here are the functions which we typically use to perform aggregations.\n",
    "  * count\n",
    "  * sum, avg\n",
    "  * min, max\n",
    "* If we want to provide aliases to the aggregated fields then we have to use `agg` after `groupBy`.\n",
    "* Let us get the count of flights for each day for the month of 200801."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Basic Transformations'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = \"/public/airtraffic_all/airtraffic-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark. \\\n",
    "    read. \\\n",
    "    parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lpad, count, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080120|      18653|\n",
      "|  20080130|      19766|\n",
      "|  20080115|      19503|\n",
      "|  20080118|      20347|\n",
      "|  20080122|      19504|\n",
      "|  20080104|      20929|\n",
      "|  20080125|      20313|\n",
      "|  20080102|      20953|\n",
      "|  20080105|      18066|\n",
      "|  20080111|      20349|\n",
      "|  20080109|      19820|\n",
      "|  20080127|      18903|\n",
      "|  20080101|      19175|\n",
      "|  20080128|      20147|\n",
      "|  20080119|      16249|\n",
      "|  20080106|      19893|\n",
      "|  20080123|      19769|\n",
      "|  20080117|      20273|\n",
      "|  20080116|      19764|\n",
      "|  20080112|      16572|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    groupBy(concat(\"year\",\n",
    "                   lpad(\"Month\", 2, \"0\"),\n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")\n",
    "                  ).alias(\"FlightDate\")\n",
    "           ). \\\n",
    "    agg(count(lit(1)).alias(\"FlightCount\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using order_items, get revenue for each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_path = '/public/retail_db/order_items'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    csv(order_items_path, \n",
    "        schema=\"\"\"\n",
    "            order_item_id INT, order_item_order_id INT,\n",
    "            order_item_product_id INT, order_item_quantity INT,\n",
    "            order_item_subtotal FLOAT, order_item_product_price FLOAT\n",
    "        \"\"\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|order_item_order_id| revenue_per_order|\n",
      "+-------------------+------------------+\n",
      "|              61793|1299.8700218200684|\n",
      "|              62015|1139.8800354003906|\n",
      "|              62680| 379.9600067138672|\n",
      "|              62985|249.89999389648438|\n",
      "|              63087|349.96001052856445|\n",
      "|              63106| 1299.910026550293|\n",
      "|              63155| 909.9200134277344|\n",
      "|              63271| 749.9800109863281|\n",
      "|              63574|199.99000549316406|\n",
      "|              63645| 1269.900032043457|\n",
      "|              63964|359.97000885009766|\n",
      "|              64121|229.99000549316406|\n",
      "|              64519|1179.9400253295898|\n",
      "|              64628| 869.9100112915039|\n",
      "|              64859| 399.9700012207031|\n",
      "|              65220| 749.9800109863281|\n",
      "|              65241| 1099.910026550293|\n",
      "|              65251|1149.9300231933594|\n",
      "|              65408| 598.9800109863281|\n",
      "|              65867| 909.8900260925293|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    groupBy('order_item_order_id'). \\\n",
    "    agg(sum('order_item_subtotal').alias('revenue_per_order')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|order_item_order_id|revenue_per_order|\n",
      "+-------------------+-----------------+\n",
      "|                148|           479.99|\n",
      "|                463|           829.92|\n",
      "|                471|           169.98|\n",
      "|                496|           441.95|\n",
      "|               1088|           249.97|\n",
      "|               1580|           299.95|\n",
      "|               1591|           439.86|\n",
      "|               1645|          1509.79|\n",
      "|               2366|           299.97|\n",
      "|               2659|           724.91|\n",
      "|               2866|           569.96|\n",
      "|               3175|           209.97|\n",
      "|               3749|           143.97|\n",
      "|               3794|           299.95|\n",
      "|               3918|           829.93|\n",
      "|               3997|           579.95|\n",
      "|               4101|           129.99|\n",
      "|               4519|            79.98|\n",
      "|               4818|           399.98|\n",
      "|               4900|           179.97|\n",
      "+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    groupBy('order_item_order_id'). \\\n",
    "    agg(round(sum('order_item_subtotal'), 2).alias('revenue_per_order')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get min and max order_item_subtotal for each order id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------------------+-----------------------+\n",
      "|order_item_order_id|revenue_per_order|order_item_subtotal_min|order_item_subtotal_max|\n",
      "+-------------------+-----------------+-----------------------+-----------------------+\n",
      "|                148|           479.99|                  100.0|                  250.0|\n",
      "|                463|           829.92|                  39.99|                 299.97|\n",
      "|                471|           169.98|                  39.99|                 129.99|\n",
      "|                496|           441.95|                  49.98|                  150.0|\n",
      "|               1088|           249.97|                 119.98|                 129.99|\n",
      "|               1580|           299.95|                 299.95|                 299.95|\n",
      "|               1591|           439.86|                  39.99|                 199.95|\n",
      "|               1645|          1509.79|                 159.96|                 399.98|\n",
      "|               2366|           299.97|                 299.97|                 299.97|\n",
      "|               2659|           724.91|                  49.98|                 299.98|\n",
      "|               2866|           569.96|                  39.99|                 299.98|\n",
      "|               3175|           209.97|                   30.0|                 179.97|\n",
      "|               3749|           143.97|                 143.97|                 143.97|\n",
      "|               3794|           299.95|                 299.95|                 299.95|\n",
      "|               3918|           829.93|                  79.98|                 299.98|\n",
      "|               3997|           579.95|                 179.97|                 399.98|\n",
      "|               4101|           129.99|                 129.99|                 129.99|\n",
      "|               4519|            79.98|                  79.98|                  79.98|\n",
      "|               4818|           399.98|                 399.98|                 399.98|\n",
      "|               4900|           179.97|                  49.98|                 129.99|\n",
      "+-------------------+-----------------+-----------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    groupBy('order_item_order_id'). \\\n",
    "    agg(\n",
    "        round(sum('order_item_subtotal'), 2).alias('revenue_per_order'),\n",
    "        min('order_item_subtotal').alias('order_item_subtotal_min'),\n",
    "        max('order_item_subtotal').alias('order_item_subtotal_max')\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
